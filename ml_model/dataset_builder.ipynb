{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "from datetime import datetime, date\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add signals to the dataset by uncommenting them\n",
    "input_features = [\n",
    "    'pct_cmnty_cli',\n",
    "    #'pct_cli',\n",
    "    #'pct_ili',\n",
    "    #'pct_cli_anosmia_ageusia',\n",
    "    #'pct_hh_cli',\n",
    "    #'pct_hh_fever',\n",
    "    #'pct_hh_sore_throat',\n",
    "    #'pct_hh_cough',\n",
    "    #'pct_hh_shortness_of_breath',\n",
    "    #'pct_hh_difficulty_breathing',\n",
    "    #'pct_self_fever',\n",
    "    #'pct_self_cough',\n",
    "    #'pct_self_shortness_of_breath',\n",
    "    #'pct_self_difficulty_breathing',\n",
    "    #'pct_self_tiredness_or_exhaustion',\n",
    "    #'pct_self_nasal_congestion',\n",
    "    #'pct_self_runny_nose',\n",
    "    #'pct_self_muscle_joint_aches',\n",
    "    #'pct_self_sore_throat',\n",
    "    #'pct_self_persistent_pain_pressure_in_chest',\n",
    "    #'pct_self_nausea_vomiting',\n",
    "    #'pct_self_diarrhea',\n",
    "    #'pct_self_anosmia_ageusia',\n",
    "    #'pct_self_other',\n",
    "    #'pct_self_none_of_above',\n",
    "    #'pct_self_multiple_symptoms',\n",
    "    #'pct_tested_and_positive',\n",
    "    #'pct_worked_outside_home',\n",
    "    #'pct_avoid_contact_all_or_most_time',\n",
    "    #'pct_contact_covid_positive'\n",
    "]\n",
    "#input_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cases data\n",
    "newcases = (pd.read_csv(\"time_series_cases.csv\")).iloc[:,1:]\n",
    "newcases.iloc[:,2:] = np.array(newcases.iloc[:,2:]) - np.array(newcases.iloc[:,1:-1])\n",
    "indnames = newcases.pop(\"FIPS\")\n",
    "newcases = newcases.rename(index = indnames,\n",
    "                           columns = lambda x: datetime.strptime(x, \"%m/%d/%y\").strftime(\"%Y-%m-%d\")).T\n",
    "newcases = newcases.rolling(7).mean().iloc[6:,:].T\n",
    "\n",
    "# Import R values, restrict to list of counties in fips.txt\n",
    "R = pd.read_csv(\"RValues.csv\", index_col='fips')\n",
    "R.pop(\"Jurisdiction\")\n",
    "fips = pd.read_table(\"fips.txt\", header = None)[0].to_numpy()\n",
    "idx = [x in fips for x in R.index]\n",
    "R = R.loc[idx,:]\n",
    "R = R.sort_index()\n",
    "\n",
    "# The index for newcases is the fips value\n",
    "idx = [x in fips for x in newcases.index]\n",
    "newcases = newcases.loc[idx,:]\n",
    "newcases = newcases.sort_index()\n",
    "newcases_dates = [datetime.strptime(x,\"%Y-%m-%d\").date() for x in newcases.columns]\n",
    "\n",
    "# Import the CMU data, limiting to entries with both gender and age_bucket as 'overall'\n",
    "cmudf = pd.read_csv(\"overall-county-smoothed.csv\").query(\"gender=='overall' & age_bucket=='overall'\")\n",
    "\n",
    "# Set the index as the identifier for the extracted features\n",
    "def extract_feature(col_name):\n",
    "    df = pd.pivot_table(cmudf, values=col_name, index='fips', columns='date',\n",
    "                        fill_value = 0, aggfunc = np.mean)\n",
    "    idx = [x in fips for x in df.index]\n",
    "    df = df.loc[idx,:]\n",
    "    df = df.sort_index()\n",
    "    return df\n",
    "\n",
    "cli = extract_feature(\"smoothed_pct_cli\")\n",
    "\n",
    "R_dates = [datetime.strptime(x,\"%Y-%m-%d\").date() for x in R.columns[1:]]\n",
    "cli_dates = [datetime.strptime(x,\"%Y-%m-%d\").date() for x in cli.columns]\n",
    "R = R.loc[:,[False,*[np.min(cli_dates) <= d <= np.max(cli_dates) for d in R_dates]]]\n",
    "\n",
    "# Ensure each dataset represents the same time period\n",
    "R_dates = [datetime.strptime(x,\"%Y-%m-%d\").date() for x in R.columns]\n",
    "newcases = newcases.loc[:,[*[np.min(cli_dates) <= d <= np.max(cli_dates) for d in newcases_dates]]]\n",
    "newcases_dates = [datetime.strptime(x,\"%Y-%m-%d\").date() for x in newcases.columns]\n",
    "newcases = newcases.loc[:,[*[np.min(R_dates) <= d <= np.max(R_dates) for d in newcases_dates]]]\n",
    "\n",
    "# Convert from Pandas to Numpy data structures\n",
    "np_newcases = newcases.to_numpy().clip(min=0)\n",
    "np_newcases = np_newcases.reshape(np_newcases.shape[0], np_newcases.shape[1], 1)\n",
    "\n",
    "np_inp = np.ndarray([*R.shape, 0], dtype = np.float32)\n",
    "# Extract and format the features identified in cell 1\n",
    "for name in input_features:\n",
    "    feature = extract_feature(f\"smoothed_{name}_weighted\")\n",
    "    feature_dates = [datetime.strptime(x,\"%Y-%m-%d\").date() for x in feature.columns]\n",
    "    feature = feature.loc[:,[np.min(R_dates) <= d <= np.max(R_dates) for d in feature_dates]]\n",
    "    np_inp = np.concatenate((np_inp, feature.to_numpy().reshape([*feature.shape,1])), axis=2)\n",
    "np_inp = np.concatenate((np_inp, np_newcases), axis=2)\n",
    "np_R = R.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import h5py as h5\n",
    "num_datasets = 100\n",
    "dataset_name = \"dataset.h5\"\n",
    "num_in, num_out, offset = 21, 21, 5\n",
    "\n",
    "def construct_set(inc_rows):\n",
    "    dataset = np.ndarray((0,num_in,np_inp.shape[2]), dtype = np.float32)\n",
    "    labels = np.ndarray((0,num_out), dtype = np.float32)\n",
    "    surges = np.ndarray((0,), dtype = np.int8)\n",
    "    date0 = datetime.strptime(newcases.columns[0], \"%Y-%m-%d\")\n",
    "    for days in range(0, np_inp.shape[1]-num_in-num_out,offset):\n",
    "        # indices for input data\n",
    "        j = 1 + np.where([days <= (datetime.strptime(d, \"%Y-%m-%d\")-date0).days < (days+num_in)\n",
    "                          for d in R.columns])[0]\n",
    "        for k in inc_rows:\n",
    "            newrow = np.array(np_inp[k,j], dtype = np.float32).reshape((1,num_in,np_inp.shape[2]))\n",
    "            scale = newrow[:,:,-1].max()\n",
    "            if scale <= 0.0:\n",
    "                print(scale)\n",
    "            newrow[:,:,-1] = (newrow[:,:,-1] / scale) * 100\n",
    "            dataset = np.append(dataset, newrow, axis = 0)\n",
    "        # indices for results\n",
    "        j = 1 + np.where([(days+num_in) <= (datetime.strptime(d, \"%Y-%m-%d\")-date0).days < (days+num_in+num_out)\n",
    "                          for d in R.columns[1:]])[0]\n",
    "        for k in inc_rows:\n",
    "            newrow = np.array(np_R[k,j], dtype = np.float32).reshape((1,labels.shape[1]))\n",
    "            labels = np.append(labels, newrow, axis = 0)\n",
    "            prev_week = (np.sum(newcases.iloc[i,j[0]-7:j[0]]))\n",
    "            cur_week = (np.sum(newcases.iloc[i,j[0]:j[0]+7]))\n",
    "            # detect surges when case are above 20 per week\n",
    "            surges = np.append(surges, [1 if prev_week > 20 and cur_week >= 1.3*prev_week else 0], axis = 0)\n",
    "    return dataset, labels, surges\n",
    "        \n",
    "with h5.File(dataset_name,\"w\") as f:\n",
    "    # Run generate datasets num_datasets times, write their results to an h5 file\n",
    "    for i in range(0, num_datasets):\n",
    "        group_name = \"group\" + str(i)\n",
    "        f.create_group(group_name)\n",
    "\n",
    "        # Generate which rows will be part of the random test/train sets\n",
    "        test_rows = []\n",
    "\n",
    "        while len(test_rows) < .1 * np_inp.shape[0]:\n",
    "            j = int(round(np.random.rand(1)[0] * np_inp.shape[0]))\n",
    "            if j not in test_rows:\n",
    "                test_rows.append(j)\n",
    "        train_rows = np.where([j not in test_rows for j in range(0, np_inp.shape[0])])[0]\n",
    "        test_rows = np.where([j in test_rows for j in range(0, np_inp.shape[0])])[0]\n",
    "\n",
    "        # Generate the test sets\n",
    "        train_dataset, train_labels, train_surges = construct_set(train_rows)\n",
    "        dataset_scales = train_dataset.max(axis=0).max(axis=0).reshape((1,1,train_dataset.shape[2]))\n",
    "        label_scale = train_labels.max()\n",
    "        train_dataset /= 100\n",
    "        train_labels /= label_scale\n",
    "\n",
    "        test_dataset, test_labels, test_surges = construct_set(test_rows)\n",
    "        test_dataset /= 100\n",
    "        test_labels /= label_scale\n",
    "        \n",
    "        file_tr_dataset = group_name + \"/train_dataset\"\n",
    "        file_tr_labels = group_name + \"/train_labels\"\n",
    "        file_te_dataset = group_name + \"/test_dataset\"\n",
    "        file_te_labels = group_name + \"/test_labels\"\n",
    "        file_scale = group_name + \"/label_scale\"\n",
    "        file_te_surge = group_name + \"/test_surge\"\n",
    "        \n",
    "        f[file_tr_dataset] = train_dataset\n",
    "        f[file_tr_labels] = train_labels\n",
    "        f[file_te_dataset] = test_dataset\n",
    "        f[file_te_labels] = test_labels\n",
    "        f[file_scale] = label_scale\n",
    "        f[file_te_surge] = test_surges\n",
    "\n",
    "    # END FOR EACH 0-num_datasets"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
